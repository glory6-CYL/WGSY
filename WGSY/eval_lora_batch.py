#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch evaluation for MedGemma + LoRA fine-tuned checkpoints (multi-disease).
åŸºäº eval_lora.py æ”¹è¿›ï¼šæ”¯æŒ batchsize=128 æ‰¹é‡æ¨ç†ï¼Œæå‡GPUåˆ©ç”¨ç‡
"""

import os
import json
import re
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import numpy as np
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------- å›ºå®šé…ç½®ï¼ˆæ–°å¢BATCH_SIZEï¼‰ -----------------
BASE_MODEL_PATH = "your_model/medgemma-base"  # æ›¿æ¢ä¸ºå®é™…åŸºç¡€æ¨¡å‹è·¯å¾„
LORA_ROOTS = [
    "your_lora/medgemma_lora_diseases"  # æ›¿æ¢ä¸ºå®é™…LoRAæ¨¡å‹æ ¹ç›®å½•
]
TEST_JSONS = {
    "your_test": "your_test.jsonl"  # æ›¿æ¢ä¸ºå®é™…æµ‹è¯•é›†JSONè·¯å¾„
}
OUTPUT_BASE = "outputs/eval_lora_batch"
BATCH_SIZE = 128  # æ ¸å¿ƒä¿®æ”¹ï¼šè®¾ç½®æ‰¹é‡å¤§å°ä¸º128
MAX_NEW_TOKENS = 256
EVAL_MAX_SAMPLES = None  # é™åˆ¶æ ·æœ¬æ•°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32

os.makedirs(OUTPUT_BASE, exist_ok=True)

# ----------------- æ ‡ç­¾æ ‡å‡†åŒ–ï¼ˆæ— ä¿®æ”¹ï¼‰ -----------------
DISEASE_CLASSES = ["WS", "DOWN", "Goldenhar", "Healthy"]
label_map = {
    "williams syndrome": "WS", "williams": "WS", "ws": "WS",
    "down syndrome": "DOWN", "trisomy 21": "DOWN", "down": "DOWN",
    "goldenhar syndrome": "Goldenhar", "oculo-auriculo-vertebral spectrum": "Goldenhar", "goldenhar": "Goldenhar",
    "healthy": "Healthy", "normal": "Healthy", "control": "Healthy",
}

def normalize_label(label):
    if not label: return "Unknown"
    label = str(label).strip().lower()
    if label in label_map: return label_map[label]
    for k in label_map:
        if k in label: return label_map[k]
    return label.capitalize()

# ----------------- JSON æå–ï¼ˆæ— ä¿®æ”¹ï¼‰ -----------------
def extract_json_safe(text):
    if not text: return None
    m = re.search(r'(\{[\s\S]*\})', text)
    if not m: return None
    cand = m.group(1)
    try:
        return json.loads(cand)
    except Exception:
        try:
            cand2 = cand.replace("'", '"').replace(",}", "}")
            return json.loads(cand2)
        except Exception:
            return None

# ----------------- message æ„é€ ï¼ˆæ— ä¿®æ”¹ï¼‰ -----------------
def build_messages_from_record(rec):
    if "messages" in rec: return rec["messages"]
    inst = rec.get("instruction") or rec.get("prompt") or ""
    return [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": inst}]}]

# ----------------- æ ¸å¿ƒè¯„ä¼°å‡½æ•°ï¼ˆæ‰¹é‡æ¨ç†ä¿®æ”¹ï¼‰ -----------------
def evaluate_lora(lora_path, test_json_path, result_dir):
    os.makedirs(result_dir, exist_ok=True)
    print(f"\n{'='*70}\nâ–¶ å¼€å§‹è¯„ä¼° LoRA: {lora_path}\nâ–¶ æµ‹è¯•é›†: {test_json_path}\nâ–¶ è¾“å‡ºè·¯å¾„: {result_dir}\nâ–¶ æ‰¹é‡å¤§å°: {BATCH_SIZE}\n{'='*70}")

    # åŠ è½½æ¨¡å‹ï¼ˆæ— ä¿®æ”¹ï¼‰
    model = AutoModelForImageTextToText.from_pretrained(
        BASE_MODEL_PATH, trust_remote_code=True, device_map="auto", dtype=DTYPE)
    model = PeftModel.from_pretrained(model, lora_path, device_map="auto")
    model.eval()
    processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    # åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆæ— ä¿®æ”¹ï¼‰
    samples = [json.loads(line) for line in open(test_json_path, "r", encoding="utf-8") if line.strip()]
    if EVAL_MAX_SAMPLES:
        samples = samples[:EVAL_MAX_SAMPLES]
    total_samples = len(samples)
    print(f"âœ… æ€»æ ·æœ¬æ•°é‡: {total_samples}")

    y_true, y_pred, preds_out, errors = [], [], [], []
    device = next(model.parameters()).device

    # æ ¸å¿ƒä¿®æ”¹1ï¼šæŒ‰æ‰¹æ¬¡éå†æ ·æœ¬ï¼ˆæ›¿ä»£å•æ ·æœ¬å¾ªç¯ï¼‰
    with tqdm(total=total_samples, desc=f"Evaluating {Path(lora_path).name}") as pbar:
        for batch_start in range(0, total_samples, BATCH_SIZE):
            # 1. åˆ‡åˆ†å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬
            batch_end = min(batch_start + BATCH_SIZE, total_samples)
            batch_samples = samples[batch_start:batch_end]
            
            # 2. æ‰¹é‡åŠ è½½å›¾ç‰‡+æ„é€ promptï¼ˆè¿‡æ»¤æ— æ•ˆæ ·æœ¬ï¼‰
            batch_texts = []  # æ‰¹é‡å­˜å‚¨promptæ–‡æœ¬
            batch_imgs = []   # æ‰¹é‡å­˜å‚¨å›¾ç‰‡å¯¹è±¡
            batch_meta = []   # æ‰¹é‡å­˜å‚¨æ ·æœ¬å…ƒä¿¡æ¯ï¼ˆidxã€true_labelï¼‰
            for idx_in_batch, rec in enumerate(batch_samples):
                global_idx = batch_start + idx_in_batch  # åŸå§‹æ ·æœ¬çš„å…¨å±€ç´¢å¼•
                try:
                    # åŠ è½½å›¾ç‰‡
                    img_path = rec.get("image") or rec.get("image_path")
                    if not img_path or not os.path.exists(img_path):
                        raise FileNotFoundError(f"Missing image: {img_path}")
                    img = Image.open(img_path).convert("RGB")
                    
                    # æ„é€ promptæ–‡æœ¬
                    messages = build_messages_from_record(rec)
                    prompt_text = processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)
                    
                    # æ”¶é›†æœ‰æ•ˆæ ·æœ¬
                    batch_texts.append(prompt_text)
                    batch_imgs.append(img)
                    batch_meta.append({"global_idx": global_idx, "true_label": normalize_label(rec.get("label"))})
                except Exception as e:
                    # å•ä¸ªæ ·æœ¬é”™è¯¯ä¸å½±å“æ‰¹æ¬¡ï¼Œç›´æ¥è®°å½•
                    errors.append({"index": global_idx, "error": str(e)})
                    continue

            # 3. è·³è¿‡ç©ºæ‰¹æ¬¡ï¼ˆæ‰€æœ‰æ ·æœ¬å‡æ— æ•ˆæ—¶ï¼‰
            if len(batch_texts) == 0:
                pbar.update(batch_end - batch_start)
                continue

            # 4. æ ¸å¿ƒä¿®æ”¹2ï¼šæ‰¹é‡å¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡ï¼ˆç”Ÿæˆbatchå¼ é‡ï¼‰
            batch = processor(
                text=batch_texts,
                images=[[img] for img in batch_imgs],  # æ­£ç¡®ï¼šæ¯ä¸ªå›¾ç‰‡ç”¨åˆ—è¡¨åŒ…è£¹ï¼ŒåŒ¹é…æ–‡æœ¬æ‰¹é‡ç»“æ„
                return_tensors="pt",
                padding=True,
                truncation=True
            )
            batch = {k: v.to(device) for k, v in batch.items()}  # è½¬ç§»åˆ°GPU

            # 5. æ‰¹é‡æ¨ç†ï¼ˆæ— ä¿®æ”¹ï¼Œæ¨¡å‹è‡ªåŠ¨å¤„ç†batchç»´åº¦ï¼‰
            with torch.no_grad():
                out_ids = model.generate(**batch, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)
            
            # 6. æ‰¹é‡è§£ç ï¼ˆå¯¹åº”æ¯ä¸ªæ ·æœ¬çš„è¾“å‡ºæ–‡æœ¬ï¼‰
            out_texts = processor.batch_decode(out_ids, skip_special_tokens=True)

            # 7. æ‰¹é‡è§£ææ ‡ç­¾ï¼ˆå¯¹åº”æ¯ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼‰
            for text_idx, (out_text, meta) in enumerate(zip(out_texts, batch_meta)):
                global_idx = meta["global_idx"]
                true_label = meta["true_label"]
                
                # è§£æé¢„æµ‹æ ‡ç­¾ï¼ˆé€»è¾‘ä¸åŸä»£ç ä¸€è‡´ï¼‰
                parsed = extract_json_safe(out_text)
                pred_label = None
                if parsed and isinstance(parsed, dict):
                    for key in ("diagnosis", "disease", "label"):
                        if key in parsed:
                            pred_label = parsed[key]
                            break
                if not pred_label:
                    low = out_text.lower()
                    for c in DISEASE_CLASSES:
                        if c.lower() in low:
                            pred_label = c
                            break
                if not pred_label:
                    pred_label = "Unknown"
                pred_label = normalize_label(pred_label)

                # è®°å½•ç»“æœ
                preds_out.append({
                    "index": global_idx,
                    "true": true_label,
                    "pred": pred_label,
                    "raw_output": out_text
                })
                if true_label in DISEASE_CLASSES:
                    y_true.append(true_label)
                    y_pred.append(pred_label)

            # 8. æ›´æ–°è¿›åº¦æ¡
            pbar.update(batch_end - batch_start)

    # åç»­æŒ‡æ ‡è®¡ç®—ã€ç»“æœä¿å­˜ï¼ˆæ— ä¿®æ”¹ï¼‰
    if not y_true:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹ï¼Œè·³è¿‡æŒ‡æ ‡è®¡ç®—ã€‚")
        return

    report = classification_report(y_true, y_pred, labels=DISEASE_CLASSES, output_dict=True, zero_division=0)
    cm = confusion_matrix(y_true, y_pred, labels=DISEASE_CLASSES)
    cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)

    plt.figure(figsize=(7, 6))
    sns.heatmap(cm_norm, annot=True, fmt=".2f", cmap="Blues", xticklabels=DISEASE_CLASSES, yticklabels=DISEASE_CLASSES)
    plt.title(f"Confusion Matrix ({Path(lora_path).name})")
    plt.tight_layout()
    plt.savefig(os.path.join(result_dir, "confusion_matrix.png"), dpi=300)
    plt.close()

    with open(os.path.join(result_dir, "metrics_summary.json"), "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    with open(os.path.join(result_dir, "predictions.jsonl"), "w", encoding="utf-8") as f:
        for p in preds_out:
            f.write(json.dumps(p, ensure_ascii=False) + "\n")
    with open(os.path.join(result_dir, "error_log.jsonl"), "w", encoding="utf-8") as f:
        for e in errors:
            f.write(json.dumps(e, ensure_ascii=False) + "\n")

    print(f"âœ… {Path(lora_path).name} è¯„ä¼°å®Œæˆå¹¶ä¿å­˜ç»“æœã€‚")

# ----------------- ä¸»æ‰¹é‡æµç¨‹ï¼ˆæ— ä¿®æ”¹ï¼‰ -----------------
if __name__ == "__main__":
    print("ğŸš€ Batch Evaluation Started")

    for lora_root in LORA_ROOTS:
        ckpts = sorted([os.path.join(lora_root, d) for d in os.listdir(lora_root) if d.startswith("checkpoint")])
        all_paths = [lora_root] + ckpts
        for lora_path in all_paths:
            for prompt_name, json_path in TEST_JSONS.items():
                result_dir = os.path.join(
                    OUTPUT_BASE, f"eval_{Path(lora_root).name}_{Path(lora_path).name}_{prompt_name}"
                )
                evaluate_lora(lora_path, json_path, result_dir)

    print("ğŸ æ‰€æœ‰æ¨¡å‹è¯„ä¼°å®Œæˆï¼")