import os
import json
import re
import argparse
from pathlib import Path
from PIL import Image
from datasets import load_from_disk
import torch

from transformers import AutoModelForImageTextToText, AutoProcessor
from peft import LoraConfig, PeftModel
from trl import SFTConfig, SFTTrainer

# ----------------- CLI -----------------
parser = argparse.ArgumentParser(description="Multi-disease LoRA fine-tuning for MedGemma")
parser.add_argument("--model-path", default="")
# ä¿®æ”¹è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†è·¯å¾„
parser.add_argument("--train-ds", default="")
parser.add_argument("--test-ds", default="")
# çˆ¶è¾“å‡ºç›®å½•ï¼ˆä¸å†æŒ‡å®šå…·ä½“å­ç›®å½•ï¼Œå­ç›®å½•ç”±å‚æ•°åŠ¨æ€ç”Ÿæˆï¼‰
parser.add_argument("--parent-output-dir", default="", help="çˆ¶ç›®å½•ï¼Œå­ç›®å½•å°†è‡ªåŠ¨åŒ…å«rå’Œdropoutå‚æ•°")
parser.add_argument("--load-in-4bit", action="store_true", help="Load in 4-bit quantization")
parser.add_argument("--use-fp16", action="store_true", help="Use FP16 training")
parser.add_argument("--merge", action="store_true", help="Optionally merge LoRA into base model (not recommended)")
parser.add_argument("--offload-folder", default=None, help="Folder for CPU/NVMe offload")
parser.add_argument("--num-epochs", type=int, default=3)
parser.add_argument("--logging-steps", type=int, default=5)
# ä¿®æ”¹è¯„ä¼°å­é›†é»˜è®¤å€¼ä¸ºNoneï¼ˆè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†ï¼‰
parser.add_argument("--eval-subset", type=int, default=None, help="Number of test samples to evaluate (use all if None)")
parser.add_argument("--max-new-tokens", type=int, default=256)
args = parser.parse_args()

# ----------------- å®šä¹‰è¦éå†çš„å‚æ•°ï¼ˆå¢åŠ r=16ï¼‰ -----------------
# å¾…éå†çš„rå€¼ï¼ˆ4ã€8ã€16ï¼‰å’Œlora_dropoutå€¼ï¼ˆ0.1ã€0.2ã€0.3ï¼‰
r_list = [4, 8, 16]  # å¢åŠ r=16çš„æƒ…å†µ
lora_dropout_list = [0.1, 0.2, 0.3]
# ç¡®ä¿çˆ¶è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(args.parent_output_dir, exist_ok=True)

# ----------------- Environment Setup -----------------
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
train_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

# ----------------- åŠ è½½æ•°æ®é›†ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤è€—æ—¶ï¼‰ -----------------
print("ğŸ“‚ Loading datasets...")
train_ds = load_from_disk(args.train_ds)
test_ds = load_from_disk(args.test_ds)
# ç¡®å®šè¯„ä¼°æ•°æ®é›†å¤§å°ï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†ï¼‰
EVAL_SUBSET = len(test_ds) if args.eval_subset is None else min(args.eval_subset, len(test_ds))

# ----------------- åµŒå¥—å¾ªç¯éå†å‚æ•°ï¼Œæ‰§è¡Œå®éªŒ -----------------
for r in r_list:
    for lora_dropout in lora_dropout_list:
        # 1. åŠ¨æ€ç”Ÿæˆå½“å‰å®éªŒçš„è¾“å‡ºç›®å½•ï¼ˆåŒ…å«rå’Œdropoutå‚æ•°ï¼‰
        current_output_dir = os.path.join(
            args.parent_output_dir, 
            f"medgemma-r{r}-dropout{lora_dropout}"  # ç›®å½•åæ ¼å¼ï¼šmedgemma-r16-dropout0.1ï¼ˆæ–°å¢æ ¼å¼ï¼‰
        )
        os.makedirs(current_output_dir, exist_ok=True)
        print(f"\n{'='*50}")
        print(f"ğŸš€ å¼€å§‹å®éªŒï¼šr={r}, lora_dropout={lora_dropout}")
        print(f"ğŸ“ å®éªŒç»“æœä¿å­˜è‡³ï¼š{current_output_dir}")
        print(f"{'='*50}")

        # 2. é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆæ¯æ¬¡å®éªŒç”¨å…¨æ–°æ¨¡å‹ï¼Œé¿å…å‚æ•°æ±¡æŸ“ï¼‰
        print(f"ğŸ”§ åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆr={r}, lora_dropout={lora_dropout}ï¼‰...")
        load_kwargs = dict(
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=train_dtype,
        )
        if args.offload_folder:
            offload_path = os.path.join(args.offload_folder, f"r{r}_dropout{lora_dropout}")
            os.makedirs(offload_path, exist_ok=True)
            load_kwargs["offload_folder"] = offload_path

        if args.load_in_4bit:
            try:
                from transformers import BitsAndBytesConfig
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                load_kwargs["quantization_config"] = bnb_config
            except ImportError:
                print("âš ï¸ bitsandbytes not available, continuing without 4bit.")

        # æ¯æ¬¡å®éªŒé‡æ–°åŠ è½½æ¨¡å‹
        model = AutoModelForImageTextToText.from_pretrained(args.model_path, **load_kwargs)
        processor = AutoProcessor.from_pretrained(args.model_path, trust_remote_code=True)
        processor.tokenizer.padding_side = "right"

        # 3. åŠ¨æ€æ„å»ºLoRAé…ç½®ï¼ˆlora_alphaä¸rä¿æŒä¸€è‡´ï¼‰
        peft_config = LoraConfig(
            lora_alpha=r,  # å…³é”®ï¼šlora_alphaå€¼è·Ÿéšå½“å‰rå€¼
            lora_dropout=lora_dropout,  # è·Ÿéšå½“å‰éå†çš„dropoutå€¼
            r=r,  # è·Ÿéšå½“å‰éå†çš„rå€¼ï¼ˆåŒ…æ‹¬æ–°å¢çš„16ï¼‰
            bias="none",
            target_modules="all-linear",
            task_type="CAUSAL_LM",
            modules_to_save=["lm_head", "embed_tokens"],
        )

        # 4. åŠ¨æ€æ„å»ºTraineré…ç½®ï¼ˆè¾“å‡ºç›®å½•ä¸ºå½“å‰å®éªŒç›®å½•ï¼‰
        sft_args = SFTConfig(
            output_dir=current_output_dir,  # è¾“å‡ºç›®å½•æŒ‡å‘å½“å‰å®éªŒç›®å½•
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=32,
            per_device_eval_batch_size=32,
            gradient_accumulation_steps=8,
            gradient_checkpointing=True,
            optim="adamw_torch_fused",
            logging_steps=args.logging_steps,
            save_strategy="epoch",
            eval_strategy="epoch",
            learning_rate=2e-4,
            fp16=args.use_fp16,
            bf16=not args.use_fp16,
            warmup_ratio=0.03,
            lr_scheduler_type="linear",
            report_to="tensorboard",
            remove_unused_columns=False,
            label_names=["labels"],
        )

        # 5. æ•°æ®æ•´ç†å‡½æ•°ï¼ˆå¤ç”¨åŸé€»è¾‘ï¼‰
        def collate_fn(examples):
            texts, images = [], []
            for ex in examples:
                img_path = ex.get("image") or ex.get("image_path")
                if not img_path or not os.path.exists(img_path):
                    raise FileNotFoundError(img_path)
                text = processor.apply_chat_template(ex["messages"], add_generation_prompt=False, tokenize=False)
                texts.append(text)
                images.append([Image.open(img_path).convert("RGB")])
            batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
            labels = batch["input_ids"].clone()
            pad = processor.tokenizer.pad_token_id
            labels[labels == pad] = -100
            labels[labels == 262144] = -100  # for safety
            batch["labels"] = labels
            return batch

        # 6. æ‰§è¡Œè®­ç»ƒ
        # è¯„ä¼°æ•°æ®é›†å¤„ç†ï¼šå¦‚æœæ˜¯å…¨é‡åˆ™ä¸åšselectï¼Œå¦åˆ™å–å‰EVAL_SUBSETä¸ª
        eval_dataset = test_ds.shuffle() if EVAL_SUBSET == len(test_ds) else test_ds.shuffle().select(range(EVAL_SUBSET))
        
        trainer = SFTTrainer(
            model=model,
            args=sft_args,
            train_dataset=train_ds,
            eval_dataset=eval_dataset if EVAL_SUBSET > 0 else None,
            peft_config=peft_config,
            processing_class=processor,
            data_collator=collate_fn,
        )

        print(f"ğŸ§  å¼€å§‹å¾®è°ƒï¼ˆr={r}, lora_dropout={lora_dropout}ï¼‰...")
        trainer.train()
        trainer.save_model(current_output_dir)
        print(f"âœ… LoRAé€‚é…å™¨å·²ä¿å­˜è‡³ï¼š{current_output_dir}")

        # 7. æ¨ç†è¯„ä¼°ï¼ˆä½¿ç”¨å®Œæ•´æµ‹è¯•é›†ï¼‰
        print(f"ğŸ” å¼€å§‹è¯„ä¼°ï¼ˆr={r}, lora_dropout={lora_dropout}ï¼‰...")
        from tqdm import tqdm

        # åŠ è½½å½“å‰å®éªŒçš„LoRAé€‚é…å™¨
        model_inf = PeftModel.from_pretrained(model, current_output_dir, device_map="auto")
        processor_inf = processor

        def extract_json_safe(text):
            if not text:
                return None
            m = re.search(r'(\{[\s\S]*\})', text)
            if not m:
                return None
            try:
                return json.loads(m.group(1))
            except Exception:
                try:
                    s = m.group(1).replace("'", '"')
                    s = re.sub(r",\s*}", "}", s)
                    return json.loads(s)
                except Exception:
                    return None

        # ç»Ÿä¸€æ ‡ç­¾æ˜ å°„è¡¨ï¼ˆç®€å†™ -> å…¨ç§°ï¼‰
        label_mapping = {
            "ws": "Williams Syndrome",
            "williams": "Williams Syndrome",
            "williams syndrome": "Williams Syndrome",
            "down": "Down Syndrome",
            "down syndrome": "Down Syndrome",
            "goldenhar": "Goldenhar Syndrome",
            "goldenhar syndrome": "Goldenhar Syndrome",
            "healthy": "Healthy",
            "normal": "Healthy",
        }

        def normalize_label(label):
            if not label:
                return "Unknown"
            label = str(label).strip().lower()
            for k, v in label_mapping.items():
                if k in label:
                    return v
            return "Unknown"

        # è¯„ä¼°ç±»åˆ«
        disease_classes = ["Williams Syndrome", "Down Syndrome", "Goldenhar Syndrome", "Healthy"]
        correct_per_class = {c: 0 for c in disease_classes}
        total_per_class = {c: 0 for c in disease_classes}
        preds = []
        error_log = []

        # æµ‹è¯•æ—¶ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†ï¼ˆç§»é™¤selecté™åˆ¶ï¼‰
        eval_subset = test_ds
        for i, ex in enumerate(tqdm(eval_subset, desc=f"Evaluating (r={r}, dropout={lora_dropout})")):
            try:
                img_path = ex.get("image") or ex.get("image_path")
                if not img_path or not os.path.exists(img_path):
                    raise FileNotFoundError(f"Image not found: {img_path}")
                img = Image.open(img_path).convert("RGB")

                prompt = processor_inf.apply_chat_template(ex["messages"], add_generation_prompt=False, tokenize=False)
                batch = processor_inf(text=[prompt], images=[[img]], return_tensors="pt", padding=True)
                batch = {k: v.to(next(model_inf.parameters()).device) for k, v in batch.items() if isinstance(v, torch.Tensor)}

                with torch.no_grad():
                    out_ids = model_inf.generate(** batch, max_new_tokens=args.max_new_tokens, do_sample=False)
                out_text = processor_inf.tokenizer.decode(out_ids[0], skip_special_tokens=True)
                parsed = extract_json_safe(out_text)

                pred_diag_raw = parsed.get("diagnosis") if parsed else "Unknown"
                pred_diag = normalize_label(pred_diag_raw)
                true_label_raw = ex.get("label")
                true_label = normalize_label(true_label_raw)

                preds.append({
                    "index": i,
                    "image": img_path,
                    "true_raw": true_label_raw,
                    "pred_raw": pred_diag_raw,
                    "true": true_label,
                    "pred": pred_diag,
                    "text": out_text,
                })

                # ç»Ÿè®¡æ­£ç¡®ç‡
                if true_label in disease_classes:
                    total_per_class[true_label] += 1
                    if pred_diag == true_label:
                        correct_per_class[true_label] += 1
            except Exception as e:
                error_log.append({"index": i, "error": str(e), "sample": ex.get("image")})
                continue

        # æ±‡æ€»å½“å‰å®éªŒç»“æœ
        print(f"\nğŸ“Š å®éªŒç»“æœï¼ˆr={r}, lora_dropout={lora_dropout}ï¼‰:")
        for c in disease_classes:
            total = total_per_class[c]
            correct = correct_per_class[c]
            acc = correct / total if total > 0 else 0.0
            print(f"{c:20s}: {acc:.3f} ({correct}/{total})")

        overall_acc = sum(correct_per_class.values()) / max(1, sum(total_per_class.values()))
        print(f"âœ… æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.3f}")

        # ä¿å­˜å½“å‰å®éªŒçš„é¢„æµ‹ç»“æœå’Œé”™è¯¯æ—¥å¿—
        preds_path = os.path.join(current_output_dir, "eval_preds_multidisease.jsonl")
        with open(preds_path, "w", encoding="utf-8") as f:
            for p in preds:
                f.write(json.dumps(p, ensure_ascii=False) + "\n")

        error_path = os.path.join(current_output_dir, "eval_error_log.jsonl")
        with open(error_path, "w", encoding="utf-8") as f:
            for e in error_log:
                f.write(json.dumps(e, ensure_ascii=False) + "\n")

        print(f"âœ… é¢„æµ‹ç»“æœä¿å­˜è‡³: {preds_path}")
        print(f"âš ï¸ é”™è¯¯æ—¥å¿—ä¿å­˜è‡³: {error_path}")
        print(f"ğŸ‰ å®éªŒï¼ˆr={r}, lora_dropout={lora_dropout}ï¼‰å®Œæˆï¼\n")

        # æ¸…ç†GPUå†…å­˜ï¼Œé¿å…åç»­å®éªŒå†…å­˜æº¢å‡º
        del model, model_inf, trainer
        torch.cuda.empty_cache()




